{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f907f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Import custom modules\n",
    "from architectures.efficientnet_unet import build_efficientnet_v2_s_unet\n",
    "from dataloaders.segmentation_dataset import SegmentationDataset, get_transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': r'path/to/your/dataset',  # Thay đổi path này\n",
    "    'image_size': 256,\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_classes': 1,\n",
    "    'model_save_path': 'efficientnet_v2s_segmentation.pth',\n",
    "    'test_output_dir': 'test_predictions'\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "train_image_dir = os.path.join(CONFIG['data_dir'], 'Train', 'Image')\n",
    "train_mask_dir = os.path.join(CONFIG['data_dir'], 'Train', 'Mask')\n",
    "val_image_dir = os.path.join(CONFIG['data_dir'], 'Val', 'Image')\n",
    "val_mask_dir = os.path.join(CONFIG['data_dir'], 'Val', 'Mask')\n",
    "test_image_dir = os.path.join(CONFIG['data_dir'], 'Test', 'Image')\n",
    "\n",
    "print(\"Data directories:\")\n",
    "print(f\"Train images: {train_image_dir}\")\n",
    "print(f\"Train masks: {train_mask_dir}\")\n",
    "print(f\"Val images: {val_image_dir}\")\n",
    "print(f\"Val masks: {val_mask_dir}\")\n",
    "print(f\"Test images: {test_image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_transform = get_transforms(CONFIG['image_size'], is_train=True)\n",
    "val_transform = get_transforms(CONFIG['image_size'], is_train=False)\n",
    "test_transform = get_transforms(CONFIG['image_size'], is_train=False)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_image_dir, train_mask_dir, train_transform)\n",
    "val_dataset = SegmentationDataset(val_image_dir, val_mask_dir, val_transform)\n",
    "test_dataset = SegmentationDataset(test_image_dir, transform=test_transform, is_test=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5349d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = build_efficientnet_v2_s_unet(\n",
    "    num_classes=CONFIG['num_classes'], \n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient\"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"Calculate IoU score\"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    num_batches = len(loader)\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        dice = dice_coefficient(outputs, masks)\n",
    "        iou = iou_score(outputs, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice\n",
    "        total_iou += iou\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Dice': f'{dice:.4f}',\n",
    "            'IoU': f'{iou:.4f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches, total_dice / num_batches, total_iou / num_batches\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    num_batches = len(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            iou = iou_score(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice\n",
    "            total_iou += iou\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Dice': f'{dice:.4f}',\n",
    "                'IoU': f'{iou:.4f}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / num_batches, total_dice / num_batches, total_iou / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b616b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_dices = []\n",
    "val_dices = []\n",
    "train_ious = []\n",
    "val_ious = []\n",
    "\n",
    "best_val_dice = 0.0\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_dice, train_iou = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_dices.append(train_dice)\n",
    "    val_dices.append(val_dice)\n",
    "    train_ious.append(train_iou)\n",
    "    val_ious.append(val_iou)\n",
    "    \n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_dice': best_val_dice,\n",
    "            'config': CONFIG\n",
    "        }, CONFIG['model_save_path'])\n",
    "        print(f\"New best model saved! Val Dice: {best_val_dice:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best Val Dice: {best_val_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139710c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Val Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Dice\n",
    "axes[1].plot(train_dices, label='Train Dice')\n",
    "axes[1].plot(val_dices, label='Val Dice')\n",
    "axes[1].set_title('Dice Coefficient')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# IoU\n",
    "axes[2].plot(train_ious, label='Train IoU')\n",
    "axes[2].plot(val_ious, label='Val IoU')\n",
    "axes[2].set_title('IoU Score')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('IoU')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80154a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for testing\n",
    "checkpoint = torch.load(CONFIG['model_save_path'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['test_output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        images = batch['image'].to(device)\n",
    "        filenames = batch['filename']\n",
    "        \n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "        \n",
    "        for i, filename in enumerate(filenames):\n",
    "            # Convert prediction to numpy\n",
    "            pred_mask = predictions[i].cpu().numpy().squeeze()\n",
    "            \n",
    "            # Convert to 0-255 range\n",
    "            pred_mask = (pred_mask * 255).astype(np.uint8)\n",
    "            \n",
    "            # Save prediction\n",
    "            output_filename = filename.replace('.jpg', '.png')\n",
    "            output_path = os.path.join(CONFIG['test_output_dir'], output_filename)\n",
    "            cv2.imwrite(output_path, pred_mask)\n",
    "\n",
    "print(f\"Predictions saved to: {CONFIG['test_output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(num_samples=5):\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Get some test samples\n",
    "    test_iter = iter(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            try:\n",
    "                batch = next(test_iter)\n",
    "                image = batch['image'].to(device)\n",
    "                filename = batch['filename'][0]\n",
    "                \n",
    "                # Get prediction\n",
    "                output = model(image)\n",
    "                prediction = torch.sigmoid(output)\n",
    "                prediction = (prediction > 0.5).float()\n",
    "                \n",
    "                # Convert to numpy for visualization\n",
    "                image_np = image[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                # Denormalize\n",
    "                image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                image_np = np.clip(image_np, 0, 1)\n",
    "                \n",
    "                pred_np = prediction[0].cpu().numpy().squeeze()\n",
    "                \n",
    "                # Plot\n",
    "                axes[i, 0].imshow(image_np)\n",
    "                axes[i, 0].set_title(f'Original: {filename}')\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                axes[i, 1].imshow(pred_np, cmap='gray')\n",
    "                axes[i, 1].set_title('Prediction')\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                axes[i, 2].imshow(image_np)\n",
    "                axes[i, 2].imshow(pred_np, alpha=0.5, cmap='Reds')\n",
    "                axes[i, 2].set_title('Overlay')\n",
    "                axes[i, 2].axis('off')\n",
    "                \n",
    "            except StopIteration:\n",
    "                break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_predictions_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcff932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: EfficientNet-V2-S UNet\")\n",
    "print(f\"Dataset: Custom Segmentation\")\n",
    "print(f\"Total epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Best validation Dice: {best_val_dice:.4f}\")\n",
    "print(f\"Image size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Model saved at: {CONFIG['model_save_path']}\")\n",
    "print(f\"Test predictions saved at: {CONFIG['test_output_dir']}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
